apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-global

data:
  prometheus.yml: |
    rule_files:
      - /etc/prometheus/*.rules

    scrape_configs:
    - job_name: 'prometheus-global'
      static_configs:
        - targets: ['localhost:9090']

    - job_name: 'prometheus-regions'
      scheme: https
      scrape_interval: 15s
      scrape_timeout: 10s

      honor_labels: true
      metrics_path: '/federate'

      params:
        'match[]':
          - '{__name__=~"^blackbox_.+"}'
          - '{__name__=~"^canary_.+"}'          
          - '{__name__=~"^datapath_.+"}'
          - '{__name__=~"^kube_.+"}'
          - '{__name__=~"up"}'
          - '{__name__=~"^container_spec_.+"}'
          - '{__name__=~"^container_cpu_.+"}'
          - '{__name__=~"^container_memory_.+"}'
          - '{__name__=~"^apiserver_.+"}'
          - '{__name__=~"^scheduler_.+"}'

      relabel_configs:
        - action: replace
          source_labels: [__address__]
          target_label: region 
          regex: prometheus.(.+).cloud.sap
          replacement: $1

        - action: replace
          source_labels: [__address__]
          target_label: cluster
          regex: prometheus.(.+).cloud.sap
          replacement: $1

      static_configs:
        - targets:
          - 'prometheus.staging.cloud.sap'
          - 'prometheus.eu-de-1.cloud.sap'
          - 'prometheus.ap-au-1.cloud.sap'

    alerting:
      alertmanagers:
      - scheme: https
        tls_config:
          ca_file: /etc/prometheus/ca.crt
        static_configs:
        - targets:
          - "prometheus-alertmanager.staging.cloud.sap"


  alert.rules: |-
    cluster_namespace_controller_pod_container:spec_memory_limit_bytes =
      sum by (cluster,namespace,controller,pod_name,container_name) (
        label_replace(
          container_spec_memory_limit_bytes{container_name!=""},
          "controller", "$1",
          "pod_name", "^(.*)-[a-z0-9]+"
        )
      )

    cluster_namespace_controller_pod_container:spec_cpu_shares =
      sum by (cluster,namespace,controller,pod_name,container_name) (
        label_replace(
          container_spec_cpu_shares{container_name!=""},
          "controller", "$1",
          "pod_name", "^(.*)-[a-z0-9]+"
        )
      )

    cluster_namespace_controller_pod_container:cpu_usage:rate =
      sum by (cluster,namespace,controller,pod_name,container_name) (
        label_replace(
          irate(
            container_cpu_usage_seconds_total{container_name!=""}[5m]
          ),
          "controller", "$1",
          "pod_name", "^(.*)-[a-z0-9]+"
        )
      )

    cluster_namespace_controller_pod_container:memory_usage:bytes =
      sum by (cluster,namespace,controller,pod_name,container_name) (
        label_replace(
          container_memory_usage_bytes{container_name!=""},
          "controller", "$1",
          "pod_name", "^(.*)-[a-z0-9]+"
        )
      )

    cluster_namespace_controller_pod_container:memory_working_set:bytes =
      sum by (cluster,namespace,controller,pod_name,container_name) (
        label_replace(
          container_memory_working_set_bytes{container_name!=""},
          "controller", "$1",
          "pod_name", "^(.*)-[a-z0-9]+"
        )
      )

    cluster_namespace_controller_pod_container:memory_rss:bytes =
      sum by (cluster,namespace,controller,pod_name,container_name) (
        label_replace(
          container_memory_rss{container_name!=""},
          "controller", "$1",
          "pod_name", "^(.*)-[a-z0-9]+"
        )
      )

    cluster_namespace_controller_pod_container:memory_cache:bytes =
      sum by (cluster,namespace,controller,pod_name,container_name) (
        label_replace(
          container_memory_cache{container_name!=""},
          "controller", "$1",
          "pod_name", "^(.*)-[a-z0-9]+"
        )
      )


    cluster_namespace_controller_pod_container:memory_pagefaults:rate =
      sum by (cluster,namespace,controller,pod_name,container_name,scope,type) (
        label_replace(
          irate(
            container_memory_failures_total{container_name!=""}[5m]
          ),
          "controller", "$1",
          "pod_name", "^(.*)-[a-z0-9]+"
        )
      )

    cluster_namespace_controller_pod_container:memory_oom:rate =
      sum by (cluster,namespace,controller,pod_name,container_name,scope,type) (
        label_replace(
          irate(
            container_memory_failcnt{container_name!=""}[5m]
          ),
          "controller", "$1",
          "pod_name", "^(.*)-[a-z0-9]+"
        )
      )

    ### Cluster resources ###

    cluster:memory_allocation:percent =
      100 * sum by (cluster) (
        container_spec_memory_limit_bytes{pod_name!=""}
      ) / sum by (cluster) (
        machine_memory_bytes
      )

    cluster:memory_used:percent =
      100 * sum by (cluster) (
        container_memory_usage_bytes{pod_name!=""}
      ) / sum by (cluster) (
        machine_memory_bytes
      )

    cluster:cpu_allocation:percent =
      100 * sum by (cluster) (
        container_spec_cpu_shares{pod_name!=""}
      ) / sum by (cluster) (
        container_spec_cpu_shares{id="/"} * on(cluster,instance) machine_cpu_cores
      )

    ### API latency ###

    # Raw metrics are in microseconds. Convert to seconds.
    cluster_resource_verb:apiserver_latency:quantile_seconds{quantile="0.99"} =
      histogram_quantile(
        0.99,
        sum by(le,cluster,job,resource,verb) (apiserver_request_latencies_bucket)
      ) / 1e6
    cluster_resource_verb:apiserver_latency:quantile_seconds{quantile="0.9"} =
      histogram_quantile(
        0.9,
        sum by(le,cluster,job,resource,verb) (apiserver_request_latencies_bucket)
      ) / 1e6
    cluster_resource_verb:apiserver_latency:quantile_seconds{quantile="0.5"} =
      histogram_quantile(
        0.5,
        sum by(le,cluster,job,resource,verb) (apiserver_request_latencies_bucket)
      ) / 1e6

    ### Scheduling latency ###

    cluster:scheduler_e2e_scheduling_latency:quantile_seconds{quantile="0.99"} =
      histogram_quantile(0.99,sum by (le,cluster) (scheduler_e2e_scheduling_latency_microseconds_bucket)) / 1e6
    cluster:scheduler_e2e_scheduling_latency:quantile_seconds{quantile="0.9"} =
      histogram_quantile(0.9,sum by (le,cluster) (scheduler_e2e_scheduling_latency_microseconds_bucket)) / 1e6
    cluster:scheduler_e2e_scheduling_latency:quantile_seconds{quantile="0.5"} =
      histogram_quantile(0.5,sum by (le,cluster) (scheduler_e2e_scheduling_latency_microseconds_bucket)) / 1e6

    cluster:scheduler_scheduling_algorithm_latency:quantile_seconds{quantile="0.99"} =
      histogram_quantile(0.99,sum by (le,cluster) (scheduler_scheduling_algorithm_latency_microseconds_bucket)) / 1e6
    cluster:scheduler_scheduling_algorithm_latency:quantile_seconds{quantile="0.9"} =
      histogram_quantile(0.9,sum by (le,cluster) (scheduler_scheduling_algorithm_latency_microseconds_bucket)) / 1e6
    cluster:scheduler_scheduling_algorithm_latency:quantile_seconds{quantile="0.5"} =
      histogram_quantile(0.5,sum by (le,cluster) (scheduler_scheduling_algorithm_latency_microseconds_bucket)) / 1e6

    cluster:scheduler_binding_latency:quantile_seconds{quantile="0.99"} =
      histogram_quantile(0.99,sum by (le,cluster) (scheduler_binding_latency_microseconds_bucket)) / 1e6
    cluster:scheduler_binding_latency:quantile_seconds{quantile="0.9"} =
      histogram_quantile(0.9,sum by (le,cluster) (scheduler_binding_latency_microseconds_bucket)) / 1e6
    cluster:scheduler_binding_latency:quantile_seconds{quantile="0.5"} =
      histogram_quantile(0.5,sum by (le,cluster) (scheduler_binding_latency_microseconds_bucket)) / 1e6

    ALERT K8SNodeDown
      IF up{job="kube-system/kubelet"} == 0
      FOR 1h
      LABELS {
        service = "k8s",
        severity = "warning"
      }
      ANNOTATIONS {
        summary = "Kubelet cannot be scraped",
        description = "Prometheus could not scrape a {{ "{{$labels.job}}" }} for more than one hour",
      }

    ALERT K8SNodeNotReady
      IF kube_node_status_ready{condition="true"} == 0
      FOR 1h
      LABELS {
        service = "k8s",
        severity = "warning",
      }
      ANNOTATIONS {
        summary = "Node status is NotReady",
        description = "The Kubelet on {{ "{{$labels.node}}" }} has not checked in with the API, or has set itself to NotReady, for more than an hour",
      }

    ALERT K8SManyNodesNotReady
      IF
        count by (cluster) (kube_node_status_ready{condition="true"} == 0) > 1
        AND
          (
            count by (cluster) (kube_node_status_ready{condition="true"} == 0)
          /
            count by (cluster) (kube_node_status_ready{condition="true"})
          ) > 0.2
      FOR 1m
      LABELS {
        service = "k8s",
        severity = "critical",
      }
      ANNOTATIONS {
        summary = "Many K8s nodes are Not Ready",
        description = "{{ "{{$value}}" }} K8s nodes (more than 10% of cluster {{ "{{$labels.cluster}}" }}) are in the NotReady state.",
      }

    ALERT K8SKubeletDown
      IF absent(up{job="kube-system/kubelet"}) or count by (cluster) (up{job="kube-system/kubelet"} == 0) / count by (cluster) (up{job="kube-system/kubelet"}) > 0.1
      FOR 1h
      LABELS {
        service = "k8s",
        severity = "critical"
      }
      ANNOTATIONS {
        summary = "Many Kubelets cannot be scraped",
        description = "Prometheus failed to scrape more than 10% of kubelets, or all Kubelets have disappeared from service discovery.",
      }

    ALERT K8SApiserverDown
      IF up{job="kube-system/apiserver"} == 0
      FOR 15m
      LABELS {
        service = "k8s",
        severity = "warning"
      }
      ANNOTATIONS {
        summary = "API server unreachable",
        description = "An API server could not be scraped.",
      }

    # Disable for non HA kubernetes setups.
    ALERT K8SApiserverDown
      IF absent({job="kube-system/apiserver"}) or (count by(cluster) (up{job="kube-system/apiserver"} == 1) < count by(cluster) (up{job="kube-system/apiserver"}))
      FOR 5m
      LABELS {
        service = "k8s",
        severity = "critical"
      }
      ANNOTATIONS {
        summary = "API server unreachable",
        description = "Prometheus failed to scrape multiple API servers, or all API servers have disappeared from service discovery.",
      }

    ALERT K8SSchedulerDown
      IF absent(up{job="kube-system/scheduler"}) or (count by(cluster) (up{job="kube-system/scheduler"} == 1) == 0)
      FOR 5m
      LABELS {
        service = "k8s",
        severity = "critical",
      }
      ANNOTATIONS {
        summary = "Scheduler is down",
        description = "There is no running K8S scheduler. New pods are not being assigned to nodes.",
      }

    ALERT K8SControllerManagerDown
      IF absent(up{job="kube-system/controller-manager"}) or (count by(cluster) (up{job="kube-system/controller-manager"} == 1) == 0)
      FOR 5m
      LABELS {
        service = "k8s",
        severity = "critical",
      }
      ANNOTATIONS {
        summary = "Controller manager is down",
        description = "There is no running K8S controller manager. Deployments and replication controllers are not making progress.",
      }

    ALERT K8SConntrackTableFull
      IF 100*node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 50
      FOR 10m
      LABELS {
        service = "k8s",
        severity = "warning"
      }
      ANNOTATIONS {
        summary = "Number of tracked connections is near the limit",
        description = "The nf_conntrack table is {{ "{{$value}}" }}% full.",
      }

    ALERT K8SConntrackTableFull
      IF 100*node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 90
      LABELS {
        service = "k8s",
        severity = "critical"
      }
      ANNOTATIONS {
        summary = "Number of tracked connections is near the limit",
        description = "The nf_conntrack table is {{ "{{$value}}"  }}% full.",
      }

    # To catch the conntrack sysctl de-tuning when it happens
    ALERT K8SConntrackTuningMissing
      IF node_nf_conntrack_udp_timeout > 10
      FOR 10m
      LABELS {
        service = "k8s",
        severity = "warning",
      }
      ANNOTATIONS {
        summary = "Node does not have the correct conntrack tunings",
        description = "Nodes keep un-setting the correct tunings, investigate when it happens.",
      }

    ALERT K8STooManyOpenFiles
      IF 100*process_open_fds{job=~"kube-system/kubelet|kube-system/apiserver"} / process_max_fds > 50
      FOR 10m
      LABELS {
        service = "k8s",
        severity = "warning"
      }
      ANNOTATIONS {
        summary = "{{ "{{$labels.job}}" }} has too many open file descriptors",
        description = "{{ "{{$labels.node}}" }} is using {{ "{{$value}}" }}% of the available file/socket descriptors.",
      }

    ALERT K8STooManyOpenFiles
      IF 100*process_open_fds{job=~"kube-system/kubelet|kube-system/apiserver"} / process_max_fds > 80
      FOR 10m
      LABELS {
        service = "k8s",
        severity = "critical"
      }
      ANNOTATIONS {
        summary = "{{ "{{$labels.job}}" }} has too many open file descriptors",
        description = "{{ "{{$labels.node}}" }} is using {{ "{{$value}}" }}% of the available file/socket descriptors.",
      }

    # Some verbs excluded because they are expected to be long-lasting:
    # WATCHLIST is long-poll, CONNECT is `kubectl exec`.
    ALERT K8SApiServerLatency
      IF histogram_quantile(
          0.99,
          sum without (instance,node,resource) (apiserver_request_latencies_bucket{verb!~"CONNECT|WATCHLIST|WATCH"})
        ) / 1e6 > 1.0
      FOR 10m
      LABELS {
        service = "k8s",
        severity = "warning"
      }
      ANNOTATIONS {
        summary = "Kubernetes apiserver latency is high",
        description = "99th percentile Latency for {{ "{{$labels.verb}}" }} requests to the kube-apiserver is higher than 1s.",
      }

    ALERT K8SApiServerEtcdAccessLatency
      IF etcd_request_latencies_summary{quantile="0.99"} / 1e6 > 1.0
      FOR 15m
      LABELS {
        service = "k8s",
        severity = "warning"
      }
      ANNOTATIONS {
        summary = "Access to etcd is slow",
        description = "99th percentile latency for apiserver to access etcd is higher than 1s.",
      }

    ALERT K8SKubeletTooManyPods
      IF kubelet_running_pod_count > 100
      LABELS {
        service = "k8s",
        severity = "warning",
      }
      ANNOTATIONS {
        summary = "Kubelet is close to pod limit",
        description = "Kubelet {{"{{$labels.instance}}"}} is running {{"{{$value}}"}} pods, close to the limit of 110",
      }

  ca.crt: |
    -----BEGIN CERTIFICATE-----
    MIIGPTCCBCWgAwIBAgIKYQ4GNwAAAAAADDANBgkqhkiG9w0BAQsFADBOMQswCQYD
    VQQGEwJERTERMA8GA1UEBwwIV2FsbGRvcmYxDzANBgNVBAoMBlNBUCBBRzEbMBkG
    A1UEAwwSU0FQIEdsb2JhbCBSb290IENBMB4XDTE1MDMxNzA5MjQ1MVoXDTI1MDMx
    NzA5MzQ1MVowRDELMAkGA1UEBhMCREUxETAPBgNVBAcMCFdhbGxkb3JmMQwwCgYD
    VQQKDANTQVAxFDASBgNVBAMMC1NBUE5ldENBX0cyMIICIjANBgkqhkiG9w0BAQEF
    AAOCAg8AMIICCgKCAgEAjuP7Hj/1nVWfsCr8M/JX90s88IhdTLaoekrxpLNJ1W27
    ECUQogQF6HCu/RFD4uIoanH0oGItbmp2p8I0XVevHXnisxQGxBdkjz+a6ZyOcEVk
    cEGTcXev1i0R+MxM8Y2WW/LGDKKkYOoVRvA5ChhTLtX2UXnBLcRdf2lMMvEHd/nn
    KWEQ47ENC+uXd6UPxzE+JqVSVaVN+NNbXBJrI1ddNdEE3/++PSAmhF7BSeNWscs7
    w0MoPwHAGMvMHe9pas1xD3RsRFQkV01XiJqqUbf1OTdYAoUoXo9orPPrO7FMfXjZ
    RbzwzFtdKRlAFnKZOVf95MKlSo8WzhffKf7pQmuabGSLqSSXzIuCpxuPlNy7kwCX
    j5m8U1xGN7L2vlalKEG27rCLx/n6ctXAaKmQo3FM+cHim3ko/mOy+9GDwGIgToX3
    5SQPnmCSR19H3nYscT06ff5lgWfBzSQmBdv//rjYkk2ZeLnTMqDNXsgT7ac6LJlj
    WXAdfdK2+gvHruf7jskio29hYRb2//ti5jD3NM6LLyovo1GOVl0uJ0NYLsmjDUAJ
    dqqNzBocy/eV3L2Ky1L6DvtcQ1otmyvroqsL5JxziP0/gRTj/t170GC/aTxjUnhs
    7vDebVOT5nffxFsZwmolzTIeOsvM4rAnMu5Gf4Mna/SsMi9w/oeXFFc/b1We1a0C
    AwEAAaOCASUwggEhMAsGA1UdDwQEAwIBBjAdBgNVHQ4EFgQUOCSvjXUS/Dg/N4MQ
    r5A8/BshWv8wHwYDVR0jBBgwFoAUg8dB/Q4mTynBuHmOhnrhv7XXagMwSwYDVR0f
    BEQwQjBAoD6gPIY6aHR0cDovL2NkcC5wa2kuY28uc2FwLmNvbS9jZHAvU0FQJTIw
    R2xvYmFsJTIwUm9vdCUyMENBLmNybDBWBggrBgEFBQcBAQRKMEgwRgYIKwYBBQUH
    MAKGOmh0dHA6Ly9haWEucGtpLmNvLnNhcC5jb20vYWlhL1NBUCUyMEdsb2JhbCUy
    MFJvb3QlMjBDQS5jcnQwGQYJKwYBBAGCNxQCBAweCgBTAHUAYgBDAEEwEgYDVR0T
    AQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsFAAOCAgEAGdBNALO509FQxcPhMCwE
    /eymAe9f2u6hXq0hMlQAuuRbpnxr0+57lcw/1eVFsT4slceh7+CHGCTCVHK1ELAd
    XQeibeQovsVx80BkugEG9PstCJpHnOAoWGjlZS2uWz89Y4O9nla+L9SCuK7tWI5Y
    +QuVhyGCD6FDIUCMlVADOLQV8Ffcm458q5S6eGViVa8Y7PNpvMyFfuUTLcUIhrZv
    eh4yjPSpz5uvQs7p/BJLXilEf3VsyXX5Q4ssibTS2aH2z7uF8gghfMvbLi7sS7oj
    XBEylxyaegwOBLtlmcbII8PoUAEAGJzdZ4kFCYjqZBMgXK9754LMpvkXDTVzy4OP
    emK5Il+t+B0VOV73T4yLamXG73qqt8QZndJ3ii7NGutv4SWhVYQ4s7MfjRwbFYlB
    z/N5eH3veBx9lJbV6uXHuNX3liGS8pNVNKPycfwlaGEbD2qZE0aZRU8OetuH1kVp
    jGqvWloPjj45iCGSCbG7FcY1gPVTEAreLjyINVH0pPve1HXcrnCV4PALT6HvoZoF
    bCuBKVgkSSoGgmasxjjjVIfMiOhkevDya52E5m0WnM1LD3ZoZzavsDSYguBP6MOV
    ViWNsVHocptphbEgdwvt3B75CDN4kf6MNZg2/t8bRhEQyK1FRy8NMeBnbRFnnEPe
    7HJNBB1ZTjnrxJAgCQgNBIQ=
    -----END CERTIFICATE-----
